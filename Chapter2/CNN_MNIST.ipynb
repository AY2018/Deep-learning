{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "053babda-2479-4077-a6db-401b8b05b4a1",
   "metadata": {},
   "source": [
    "# MNIST : RdN - CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b70855f9-267e-4c45-a5db-fe7a51595f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "697ea7ed-e2a4-4b71-8b3a-7baf45bf27f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load nos données \n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d7c63c4-a01b-4f85-94d6-efac10dceb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparer les données \n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels, 10)\n",
    "test_labels = to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f168a-628e-4758-b3a5-2ccd901a681f",
   "metadata": {},
   "source": [
    "Un CNN typique comprend plusieurs types de couches :\n",
    "\n",
    "**Couches de convolution** : Ces couches utilisent un ensemble de filtres qui effectuent une opération de convolution sur l'entrée pour extraire des caractéristiques. Chaque filtre produit une **carte d'activation** qui réagit à la présence de caractéristiques spécifiques à un emplacement donné dans l'entrée.\n",
    "\n",
    "```python\n",
    "layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "```\n",
    "\n",
    "**Couches de pooling** (sous-échantillonnage) : Après la convolution, les couches de pooling sont utilisées pour réduire les dimensions spatiales des cartes d'activation. = aide à rendre le modèle plus robuste aux variations de position des caractéristiques dans les entrées et réduit la quantité de calculs nécessaires.\n",
    "\n",
    "```python\n",
    "layers.MaxPooling2D((2, 2))\n",
    "```\n",
    "\n",
    "Couches entièrement connectées : Après plusieurs couches de convolution et de pooling, les cartes d'activation sont aplaties en un vecteur et passées à une ou plusieurs couches entièrement connectées (similaires à celles des MLP) pour la classification ou la régression.\n",
    "\n",
    "```python\n",
    "layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "```\n",
    "\n",
    "Avantages :\n",
    "- Invariance spatiale : Grâce aux couches de pooling, les CNN sont capables de détecter des caractéristiques indépendamment de leur position dans l'entrée, ce qui est crucial pour des tâches comme la reconnaissance d'objets.\n",
    "- Excellente performance : Sur des tâches de vision par ordinateur et autres tâches similaires, les CNN surpassent souvent les autres types de réseaux de neurones et sont le choix standard pour la détection et la reconnaissance d'objets, la segmentation d'images, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11ef829f-1e57-484e-a2a0-b74e46c4624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayoub/miniforge3/envs/keras/lib/python3.9/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# On build le modèle CNN\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a14180b-9f74-4acf-8684-f3aa05fb88cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d819ca49-34dc-4a17-b3f0-bbe283c5c9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.8688 - loss: 0.4252\n",
      "Epoch 2/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.9828 - loss: 0.0537\n",
      "Epoch 3/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9894 - loss: 0.0344\n",
      "Epoch 4/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9915 - loss: 0.0255\n",
      "Epoch 5/5\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 12ms/step - accuracy: 0.9930 - loss: 0.0215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2977148b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a4ab419-24d3-497e-9023-804b91453582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9897 - loss: 0.0306\n",
      "Test accuracy: 0.9916999936103821\n"
     ]
    }
   ],
   "source": [
    "# Tester le modèle\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2b6e8-b6d5-4d8a-b46d-0fc438cca5c7",
   "metadata": {},
   "source": [
    "# Pour télécharger le modèle : (voir fichier TrainedModel.ipynb pour utilisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bd084fa-7c1f-4584-99ee-c8c015354cc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=cnn_model_mnist.model3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcnn_model_mnist.model3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/keras/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/keras/lib/python3.9/site-packages/keras/src/saving/saving_api.py:106\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m     legacy_h5_format\u001b[38;5;241m.\u001b[39msave_model_to_hdf5(\n\u001b[1;32m    103\u001b[0m         model, filepath, overwrite, include_optimizer\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filepath extension for saving. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease add either a `.keras` extension for the native Keras \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat (recommended) or a `.h5` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `model.export(filepath)` if you want to export a SavedModel \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor use with TFLite/TFServing/etc. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=cnn_model_mnist.model3."
     ]
    }
   ],
   "source": [
    "model.save('cnn_model_mnist.model3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Keras Env",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
